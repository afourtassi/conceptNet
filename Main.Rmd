---
output:
  html_document: default
  pdf_document: default
---
# Basic Network Analysis of Wordbank Data

## Build Networks

List of libraries
```{r echo=TRUE, warning=FALSE, message=FALSE}
library(purrr) # fp library
library(readr) # read text data files (csv, etc.)
library(ggplot2)
library(langcog) # useful things used by Language and Cognition Lab
library(boot) # functions for bootstrapping
library(dplyr) # for working with datafram-like objects
library(tidyr) # for data tidying
library(wordbankr) # access to wordbank data
library(directlabels) # direct labeling of data in plots
library(stringr) # common string operations
library(lmtest) #  diagnostic checking in linear regression models
library(rwebppl) # interface to probabilistic programming language WebPPL
library(jsonlite) # JSON parser
library(nlme) # gaussian linear and nonlinear mixed-effects models
library(feather) # read and write fast lightweight binary columnar data
library(broom) # converts R function outputs to tidy data frames
library(HDInterval) # calculate highest density intervals of objects w/ pdf
library(BBmisc) # misc helper functions
library(igraph) # network analysis and visualization
library(knitr) # general-purpose tool for dynamic report generation
library(lemon) # functions for working w/ ggplot2 and knitr extensions
library(tibble) # modern dataframes
```


Import helper functions and configure RMarkdown
```{r echo=TRUE, warning=FALSE, message=FALSE}
source(paste(getwd(),"/helpers/all_helper.r",sep = ""), chdir = T)

knit_print.data.frame <- lemon_print # use kable when displaying tibbles in knit
```


Create pairs 
```{r echo=TRUE, warning=FALSE, message=FALSE}
#Import data from wordbank in a format that will be useful for us (especially when we will do development in time).

#The format: for each age (in months, it starts with 16, 17,..) list all words that have NOT yet been acquired  (acquisition is defined by the criterion that 50% of the children can produce the word). The "defintion" is the word, the "unilemma"" is the translation in English (but since we are using only English at this point, both are the same). "learned" is 1 when the word is acquired at that month, and 0 when the word is not aquired at that month. Thus, words with learned=1 at month n, will disapear from month n+1

wb_data <- make_aoa_dataframe(lang="English (American)", lang_form = "WS", lex_class = "nouns")


#extract the first age (it is 16 month in English)
first_age<- wb_data$age[1]

#Extract the list of all uni_lemmas (like Hills, we will start with the analysis of all words first)
lemma_list<- wb_data %>%
  trim_all_unilemma() %>% #We do naive triming at this point, this means we are ignoring homophone/polysemy (chiken (food), chiken (animals), etc,.. but we will have to be careful in our choices if we are preparing later for publication
  filter(age==first_age) %>% #Since the first month in our format is the month when all words are stil present 
  select(item, uni_lemma) 
    
    # list of definitions (we don't need them at this point)
    def_list<- wb_data %>%
      trim_all_definition() %>% 
      filter(age==first_age) %>%
      select(item, definition)
    
#Make list of pairs for associative data
#The output: all pairs of words (first is named "item"" and second is named "pair"), link =0 (no link), =1 (there is a link)
assoc_pairs <- make_assoc_pairs(lemma_list = lemma_list)

#Make list of pairs for MacRae features
#The output: same as above, but here instead of link, we have "shared" which specify the number of shared links 
feature_pairs <- make_feature_pairs(lemma_list = lemma_list)
```

Build networks
```{r}
assoc_links <- assoc_pairs %>%
  filter(link==1) %>%
  select(item, pair, item.definition, pair.definition)

feature_links <- feature_pairs %>%
  filter(shared > 0) %>% # arbitrary, what does the # of shared links represent?
  select(item, pair, item.definition, pair.definition)

assoc_network <- graph_from_data_frame(assoc_links, directed=FALSE, vertices=lemma_list) %>%
  simplify() %>%
  set.graph.attribute("name", "Assoc") # maybe rewrite as g$name <- "Name"

feature_network <- graph_from_data_frame(feature_links, directed=FALSE, vertices=lemma_list) %>%
  simplify() %>%
  set.graph.attribute("name", "Feature")

networks <- list(assoc_network, feature_network)
```

## Network Analysis

### Large-Scale Structure of Networks

a la Steyvers 2005

```{r, results='asis'}
network_properties <- tibble(
  names = map_chr(networks, ~ .x$name),
  vertices = map_int(networks, vcount),
  edges = map_dbl(networks, ecount) %>%
    as.integer(), # for some reason igraph ecount returns double
  avg_degree = map(networks, degree) %>%
    map_dbl(mean),
  avg_shortest_path = map_dbl(networks, mean_distance),
  diameter = map_dbl(networks, diameter) %>%
    as.integer(),
  clustering_coefficient = map_dbl(networks, transitivity),
  avg_shortest_path_random = 0, # either estimate or calculate values for random nets
  diameter_random = 0,
  clustering_coefficient_random = 0
)

network_properties # improve formatting
```

Degree Distribution
```{r}
assoc_degree <- tibble(k = 1:length(degree_distribution(assoc_network)),
                       p = degree_distribution(assoc_network))
feature_degree <- tibble(k = 1:length(degree_distribution(feature_network)),
                       p = degree_distribution(feature_network))

ggplot(data = assoc_degree, aes(x = k, y = p)) + geom_line()
ggplot(data = feature_degree, aes(x = k, y = p)) + geom_line()
```
