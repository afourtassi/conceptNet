---
title: "The development of abstract concepts in children's early lexical networks"

author-information: > 
    \author{{\large \bf Author 1} \\ \texttt{author1@university.edu} \\ Department of Psychology \\ Some University
    \And {\large \bf Author 2} \\ \texttt{author1@university.edu} \\ Department of Psychology \\ Some University}

abstract: 
    "How do children learn abstract concepts such as animal vs. artifact? Previous research has suggested that such concepts can partly be derived using cues from the language children hear around them. Following this suggestion, we propose a model where we represent the children's developing lexicon as an evolving network. The nodes of this network are based on vocabulary knowledge as reported by parents, and the edges between pairs of nodes are based on the probability of their co-occurrence in a corpus of child-directed speech. We found that several abstract categories can be identified as the dense components in such networks. In addition, these categories develop simultaneously, rather than sequentially, thanks to the children's word learning trajectory which favors the exploration of the global conceptual space."
    

final_submission: no
# If yes, uncomment line below and add your own ID
# eaclpaperid: "1234"

# Uncomment and change if you want a smaller titlebox; minimum length is 5cm
# titlebox_length: "5cm"

output: acl2017::acl_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(dplyr)
library(tidyr)
library(plyr)
library(broom)
#library(tidyr)
library(purrr)
library(langcog)
library(ggplot2)
library(grid)
library(lme4)
library(lmerTest)
library(ggthemes)
library(ggplot2)
library(xtable)
```

#Introduction:
<!--Different factors influence word learning. But here we are interested in how this order may influnce conceptual development
-The role of language is particularly important in the acquisition of higher levels concepts (because shared features are not necessarily perceived). One idea is to used the way words are distributed in child directed speech to infer the underlying categories  

Concepts are the building blocks of ideas. For example, the thought "birds are animals" requires the knowledge of both the concept "birds" and the concept "animal". They enable inductive reasoning and predictive inferences which guide behavior and explanation (Murphy, 2002). 
-->
One of the central challenges in cognitive development is to understand how concepts develop \cite{carey2009,keil1992,gopnik1997}. Of particular interest is the case of abstract concepts which have non-obvious shared properties such as "animal" and "artifact". For example, a cat and a bird are perceptually quite different but they share some fundamental properties (e.g., breathing, feeding, and reproducing) which make them animals (as opposed to artifacts). In such cases, learning requires in part cultural/linguistic cues which provide information beyond what can be obtained through the senses \cite{gelman2009,harris2012,csibra2009}.

One way children's conceptual learning can benefit from the language they hear around them is through word co-occurrence. For example, one can learn an abstract concept (e.g., animal) simply by observing how its instances (e.g., "cat" and "bird") go together in speech. Indeed, previous work has shown that the caregiverâ€™s input contains rich co-occurrence information about various abstract concepts \cite{huebner2018}. This work, however, has explored the conceptual space from an adult perspective (using the words uttered by the caregivers).  Here we explore how abstract concepts may develop from the children's perspective, investigating how their word learning trajectory influences the higher-level organization.

<!--On the other hand, children can track co-occurrence statistics \cite{saffran1996} and there is evidence that they use co-occurrence to make category-based generalization \cite{fisher2011,matlen2015}. 
Here we investigate the way abstract concepts may develop from the interaction of the children developing lexicon and co-occurrence information in the caregiver's speech. 
-->
We study development in light of two hypothetical models. On the first, called the simultaneous model, learning starts by exploring the global conceptual structure; categories are refined simultaneously over development. On the second, called the sequential model, learning starts by exploring a small region of the conceptual space (e.g., the category "animals") and only after the refinement of this category, does the learner move to another.

The paper is organized as follows. First, we describe the research strategy. In brief, we represented the developing lexicon as an evolving network and we used word co-occurrence in parent speech as a measure of words' relatedness. We operationalized abstract concepts as the highly interconnected components of the network.  Second, we explore how the pattern of children's word learning influences higher-level conceptual development, and whether this development corresponds to a simultaneous or a sequential conceptual growth. 

```{r network, fig.env = "figure*", fig.pos = "h", fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.width=5, fig.height=8, fig.cap = "Network obtained using a sample of nouns in CDI data (nodes), and co-occurrence-based similarity from a corpus of child-directed speech (edges). Colors indicate highly interconnected clusters identified using unsupervised network community detection. The clusters correspond, overall, to four higher-level concepts: animal, food, clothes, and artifacts."}

img <- png::readPNG("figs/graph.png")
grid::grid.raster(img)

```

# Data and Methods

## Constructing lexical networks

The networks' nodes were nouns from Wordbank \cite{frank2017}, an open repository aggregating cross-linguistic developmental data of the MacArthur-Bates Communicative Development Inventory (CDI), a parent report vocabulary checklist, Toddler version \cite{fenson94}. Pairs of nouns were linked by weighted edges representing their semantic similarity derived based on co-occurrence in the corpus of child-directed speech CHILDES \cite{macwhinney2014}, using Word2Vec algorithm \cite{mikolov2013}. 

First, we constructed the end-state network based on a subset of CDI nouns named "uni_lemmas" in the WordBank database. We used this subset since its items are translated across several languages, allowing us to account for cross-linguistic variability. We used the ten languages studied in \newcite{braginsky}. The size of this subset varied from XX in English (representing X% of total nouns in this language) to XX in XX (representing X% of total nouns). Second, in order to study development towards the end-state, we constructed a different network at each month, based on the nouns that have been learned by that month. 

## Identifying abstract concepts in a network 

We assume that abstract concepts correspond to clusters of highly interconnected nodes in the networks. We identified such clusters using  WalkTrap \cite{pons2006}, an unsupervised community detection algorithm based on the fact that a random walker tends to be trapped in dense parts of a network. Figure \ref{fig:network} shows the outcome of cluster identification in the end-state network. The algorithm obtained four major clusters corresponding to the categories of clothes, food, animal and artifacts. We refer to this end-state clustering as $\mathcal{C}^*$. To examine developmental change in the conceptual organization, we ran the cluster identification algorithm at each month of acquisition $t$, and we compared the resulting clustering, noted $\mathcal{C}_t$, to that of the end-state $\mathcal{C}^*$. The method of this comparison is detailed below.


```{r results, fig.env = "figure*", fig.pos = "h", fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.width=7, fig.height=3, fig.cap = "Mean precision and recall scores obtained through comparing the end-state clustering to clusterings at different months of acquisition, across different languages. Colors indicates real and hypothetical word sampling mechanisms. Errors bars represent 95\\% confidence intervals."}

measurements <- readRDS("data/measurements.rds")

all <- measurements %>%
  filter(measure %in% c("pair_precision_to_last", "pair_recall_to_last"),
         randomization != "to_nearest_aoa",
         n_clusters != '5') #%>%
  #group_by(n_clusters, age, measure, randomization) %>%
  #summarise(mean = mean(value))

all_sum <- all %>%
  group_by(measure, randomization, age) %>%
  multi_boot_standard(col = "value", na.rm = TRUE)
  #summarise(myMean = mean(value, na.rm = TRUE))

all_sum$measure <- mapvalues(all_sum$measure, from = c("pair_precision_to_last", "pair_recall_to_last"), to = c("Precision", "Recall"))
all_sum$randomization <- mapvalues(all_sum$randomization, from = c("none", "random_aoa", "within_last_clustering"), to = c("Real", "Exploration","Exploitation"))

all_sum_corr <- all_sum %>%
  as.data.frame() %>%
  select(-ci_lower, -ci_upper) %>%
  spread(randomization, mean) %>%
  group_by(measure) %>%
  dplyr::summarise(sim = round(cor(Real, Exploration)^2, 2),
            seq = round(cor(Real, Exploitation)^2, 2))


r_sim_prec <- all_sum_corr$sim[which(all_sum_corr$measure=="Precision")]
r_sim_rec <- all_sum_corr$sim[which(all_sum_corr$measure=="Recall")]

r_seq_prec <- all_sum_corr$seq[which(all_sum_corr$measure=="Precision")]
r_seq_rec <- all_sum_corr$seq[which(all_sum_corr$measure=="Recall")]


all_sum$randomization <- factor(all_sum$randomization, levels = c("Real", "Exploration","Exploitation"))


ggplot(data=all_sum, aes(x=age, y=mean, col=randomization)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = position_dodge(width = .1),
                  size=0.1)+
  geom_line()+
  xlab("Age") + ylab("Mean score") +
  #geom_smooth(method="lm", formula = y ~ poly(x, 3)) +
  #geom_smooth(method="lm", formula = y ~ log(x)) +
  theme_few() + 
  theme(aspect.ratio = 0.7,
        legend.title = element_blank())+
  facet_grid(.~measure) +
  theme(legend.position="bottom")
```

## Measuring conceptual development 

We measure conceptual development by comparing $\mathcal{C}_t$ to $\mathcal{C}^*$ across time. We used a standard method in clustering comparison, which is based on counting word pairs on which the two clusterings agree or disagree \cite{rand1971,hubert1985}. A pair of words learned by month $t$ can fall under one of the four following cases: 

\begin{enumerate}

  \item True positives $tp(\mathcal{C}_t)$: pairs that are placed in the same cluster under $\mathcal{C}_t$ and in the same cluster under $\mathcal{C}^*$.
  \item True negatives $tn(\mathcal{C}_t)$: pairs placed in different clusters under $\mathcal{C}_t$ and in different clusters under $\mathcal{C}^*$.
  \item False positive $fp(\mathcal{C}_t)$: pairs placed in the same cluster under $\mathcal{C}_t$ and in different clusters under $\mathcal{C}^*$.
  \item False negatives $fn(\mathcal{C}_t)$: pairs placed in different clusters under $\mathcal{C}_t$ and in the same cluster under $\mathcal{C}^*$.
  
\end{enumerate}

We quantify clustering comparison using precision $P(\mathcal{C}_t)$ and recall $R(\mathcal{C}_t)$, defined as follows:

$$
P(\mathcal{C}_t) = \frac{|tp(\mathcal{C}_t)|}{|tp(\mathcal{C}_t)| + |fp(\mathcal{C}_t)|}
$$

$$
R(\mathcal{C}_t) = \frac{|tp(\mathcal{C}_t)|}{|tp(\mathcal{C}_t)| + |fn(\mathcal{C}_t)|}
$$
We made this comparison using different degrees of clustering granularity. More precisely, we fixed the same number of clusters for both $\mathcal{C}_t$ and $\mathcal{C}^*$, and we varied this number from two to four clusters. We did not use the trivial case of one cluster, nor did we did use more than four clusters, since this number was optimal for the largest network (i.e., the end-state network) based on the modularity maximization criterion \cite{newman2006}.

<!--We investigate whether there are systematic cross-lingusitc variations in the order of aquisition of words that make up this vocabulary, and crucially, whether such variation influences the induced conceptual organization at different points in time. In fact, differences in the order of acqusition of words do not necessarily give rise to different conceptual organization. Imagine that two languages vary in whether "cow" or "dog" is acquired first. This difference will not change the induced conceptual organization across time since both "dog" and "cow" are instances of the same high-level concept "animal". -->


<!--## Features
We used two sources of information which represent two ways words may be related to one another in the semantic network. The first source was the MacRae features (McRae et al., 2005). These features were collected by giving adult participants a set of nouns and prompting them to provide various kinds of properties (perceptual, functional, encyclopedic, and taxonomic). We followed Hills et al. 2010 in excluding the encyclopedic and taxonomic properties as their role may not be important at this stage of development. The other source was the semantic similarity derived based on co-occurrence in the corpus of CHILDES, using word2vec.
-->


<!--
## Small World properties
We test whether the networks display the so-called "small-world" properties similar to other semantic and real-world networks (Steyvers & Tenenbaum, 2005; Watts & Strogatz, 1998). Small world properties are characterized with the average clustering coefficient $C$ and the average shortest path $L$. The former measures the extent to which the network is clustered, i.e., made of highly connectned sub-networks, whereas the latter measures the typical separation between tow nodes in the network. A network is small-word if it has a higher clustering coeffient comapred to a randomly connected network of the same size $C \gg C_{random}$, while still having a shorest path length as small as the one typically observed in random networks, that is $L \approx L_{random}$. 
-->

<!--Both Precison and Recall converge to 1 (perfect score) as $\mathcal{C}_t$ becomes more and more similar to $\mathcal{C}^*$. If precision starts low before converging to 1 (as opposed to being a constant at 1), this pattern would indicate that some pairs that should be differentiated are initially lumped together, suggesting a process of "differentiation" over development. Similarly, if we observe an increase in Recall, this pattern would indicate that some pairs that should be associated are initially differentiate, suggesting a process of "coelescence" over development. -->

## Learning mechanisms

We examined how abstract concepts develop under the children's real word learning trajectory. To construct this trajectory, we used the normative age of acquisition, that is, the age at which a word is produced by at least 50% of children in each language \cite{goodman2008}. We compared this development to the development induced by a simultaneous model and the development induced by a sequential model.

The simultaneous model was instantiated as a uniform sampling across time from the end-state vocabulary. The sequential model had the additional constraint of sampling from one category at a time: the first word is selected randomly from one cluster, subsequent words are sampled from the same cluster. After all words from this cluster are used, a word from a different cluster is chosen, and the same process is repeated until all clusters are covered.\footnote{Note that the way we instantiated the sequential model is not fully unsupervised, but we were more interested in modeling extreme cases to which real learning can be compared.}
<!--
Since we are interested in the cross-linguistic study of development, we started by investigating the degree to which languages vary in their normative order of word learning. To this end, we calulated the pairwise Kendall rank correlation coefficient between languages, and we comapre it to the same measure obtained by comparing trajectories in different languages to random orderings.  The corresponding distributions, shown in Figure XX, do not overlap indicating that languages are more similar to each other than to random word orderings. The between-language distribution also shows there to be susbtantial cross-linguitc variability since the values are smaller than 1 (i.e., perfect correlation).
-->

<!--
```{r fig.env = "figure", fig.pos = "H", fig.align = "center", set.cap.width=T, fig.width=3, fig.height=4, num.cols.cap=2, fig.cap = "bla bla"}

cor_all <- feather::read_feather("../saved_data/data_corr_all.feather")

ggplot(cor_all, 
      aes(x = tau, fill=data)) +
  geom_histogram(aes(y = (..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]),
                 alpha=0.5,
                 binwidth=0.05)+
  #scale_x_log10() +
  theme_few() + 
  theme(legend.title = element_blank(),
      legend.text=element_text(size=7.5),
      axis.text = element_text(size = 7.5),
      strip.text = element_text(size = 7.5),
      axis.title = element_text(size = 7.5),
      aspect.ratio = 0.7
      )  +  #facet_grid(Segmentation ~ language)+
  scale_y_continuous(labels = scales::percent)+
  xlab("pairwise correlation") +ylab("Count") +
  theme(legend.position="bottom")
  

```
-->
# Results

Figure \ref{fig:results} shows the scores obtained through comparing $\mathcal{C}^*$ to $\mathcal{C}_t$ at different points in time $t$. For the real word learning trajectory, both precision and recall start relatively low, indicating that the induced conceptual organization is initially quite different from that of the end-state.  Both measures converge towards 1 (i.e., perfect score) as $\mathcal{C}_t$ becomes more and more similar to $\mathcal{C}^*$.

The simultaneous model mimics closely the patterns of real conceptual development, explaining almost all the variance in mean precision  ($R^2 =$ `r r_sim_prec`) and recall ($R^2 =$ `r r_sim_rec`). In contrast, the sequential model had generally a higher precision, i.e., it induced less false positive pairs. This result is due to the fact that we sampled instances from a same category. However, the same model had generally lower recall scores, i.e., it induced more false negative pairs. This second result was due to the fact that sampling from a same category leads to clusterings that are finer in their conceptual granularity than the end-state. As a consequence of this discrepancy with respect to real development, the sequential model explained less variance than the simultaneous model did in both the mean precision ($R^2 =$ `r r_seq_prec`) and recall ($R^2 =$ `r r_seq_rec`).

# Discussion

Can children learn abstract concepts based on word co-occurrence in the language they hear around them? Previous work has shown that child-directed speech contains information about several abstract concepts \cite{huebner2018}. Here we investigated when and how this information becomes available to children as their lexical network grows. We found that even with a small lexicon, several high-level concepts such as "animal", "artifact", "food" and "clothes" emerge bottom-up as clusters of highly interconnected nodes in the network. Furthermore,  compared with a model that posited sequential learning, we found that these categories tended to emerge in concert with one another.

<!--thanks to the children's word learning trajectory which favor the exploration of the global conceptual landscape rather than the exploitation and refinement of one specific category at a time. -->

The development of the higher-level conceptual structure seems to be unaffected by the order with which words are acquired (as long as this order approximates a uniform sampling from the end-state lexicon), suggesting that the process of conceptual development can accommodate a wide range of word learning trajectories without a qualitative change in the higher-level organization. For example, whether acquisition starts first with the words "cat" and "banana" or with the words "cow" and "potato" does not qualitatively affect the higher-level organization involving "animal" and "food". This property is important as it suggests, for instance, that development is resilient to variability in the children's linguistic input \cite{slobin2014,hart1995}.

Developmental changes were captured by precision and recall. The increase in precision means that false positives decrease over time: some word pairs that are initially lumped together in a same category, are eventually differentiated. Similarly, the increase in recall means that false negatives decrease, that is, some word pairs that are initially distinct, become eventually subsumed by a same category. These patterns suggest a process of conceptual reorganization involving both "differentiation" and "coalescence" as was suggested in the developmental literature \cite{carey2009}.

That said, these developmental changes were not necessarily related to specific concepts (since the patterns were similar when we randomized the order of word learning). Instead, this finding suggests that differentiation and coalescence of word pairs in our data are related to the change in the vocabulary size across development: As more words are added to their lexical network, learners may approximate better the underlying conceptual organization of the mature lexicon and would make fewer categorization errors. Indeed, research in network science indicates that properties of a real network become more distorted as the size of a sampled sub-network decreases \cite{leskovec2006}.

One limitation of this study is that we used the normative age of acquisition, computed using different children at different age groups. This choice was due to the cross-sectional nature of available CDI data. Though such a measure has been widely used to study important aspects of the early lexical networks \cite{hills2009,stella2017,storkel2009}, it only applies at the population level. In our case, though we found that concepts develop simultaneously, individual children may display, at least locally, a sequential-like behavior. For example, prior knowledge about dinosaurs may enable the learning of new dinosaur-related words more easily \cite{chi1983}.

In sum, this work provided a quantitative account of how abstract concepts can emerge from the interaction of the children's emerging vocabulary and the properties of their linguistic input. One important direction for future work is to investigate the extent to which the correlational findings obtained in this study (e.g., the identity of categories formed across development or the fact that categorization errors decrease with the size of the lexicon) can be corroborated by controlled behavioral experiments. 

<!--

leading to gradual improvements in the approximation of the underlying conceptual organization. More precisely, words are placed in a same cluster if they are more similar to one another than to words in a different clusters. However, this *relative* similarity depends on the existing items and their number. In particular, the larger the vocabulary, the better learners can approximate the underlying relative simialrity distribution of the mature lexicon, and the less categorization errors they make.

We compare the developmental patterns obtained with the normative trajectory to those obtained with the random trajectory. We demonstrated above that these trajectories are not correlated (Figure XX). Therefore, a priori it could have been the case that the normative trajectory has effects on higher-level conceptual organization above and beyond the effects induced by the random trajectory. Instead, Figure XX shows that the precsion and recall scores are strikingly similar in both trajectories across development. Thus, high level conceptual organizaton is quite robust to the order of word acquisition. To illustrate, acquisition may start with the word "dog" or with the word "cat", but this difference in the order of acquisition will not affect the induced super-ordinate category "animal".










<!--








We investigte whether the real conceptual development resembles more a pattern of exploitation- and exploration-based sampling.  

The real word leanrining trajectory induces a conceptual organization which starts different from the end-state (lower precion and recall scores), and which becomes increasingly similar to the end-state 


The results are shown for the real trajectory, as well





First we compare the trajectory of word learning to the trajectories that result from the two sampling mechanisms. Second, we examine the extent to which these trajectories induce different patterns of conceptual development.

Results are shown in Figure XX. We used mixed-effect linear regressions to examine how the trajectory of word learning compares to the two sampling mechanisms in the way they influence conceptual development.  


Furthermore, the fact that normative and random trajectories have almost identical effects on the higher-level organization suggests that the observed developmental patterns (i.e., differentiation and coalescence of pairs of words) are not due to some specific order in word acquisition. Rather, they are due to the increase in the vocabulary size across development, leading to gradual improvements in the approximation of the underlying conceptual organization. More precisely, words are placed in a same cluster if they are more similar to one another than to words in a different clusters. However, this *relative* similarity depends on the existing items and their number. In particular, the larger the vocabulary, the better learners can approximate the underlying relative simialrity distribution of the mature lexicon, and the less categorization errors they make.

Next, we compare the normative and category-based learning (why?). Precsion is higher in the category-based trajectory, especially for earlier months. In contrast, the recall is lower. This pattern is due to the fact that learning is assumed to proceed based on conceptual similarity, e.g., if learning starts with an animal, then the next word will also be an animal. This learning produces less false postives because it samples instances from the same category (hence the high precison). Nevertheless, it also produces more false negative because it creates categories that are finer in their conceptual granualrity than the end-state (hence the low recall). 


# Conclusion
This results suggests that real (normative) learning influences early conceptual development in a way that mimics random sampling and contrasts with category-based sampling. This suggests that early word learning favors exploration of the entire conceptual domain, rath

<!--
thus favoring exploration and which differ from category-based sampling. 


conceptual development show different properties when word learning deviate 


learning proceeds according to a similarity principle which sample words from the same category.  




In contrast, the recall is lower. Category-based trajectory does not show a clear differentia




Finally, mixed effects model and cross-linguistic discussion,....


more frequent it is to approximate the end-state organization and to misplace words in . This phenomenon is similar to 



The similarity of the scores in both trajectories also indicate that the increase in precision are recall are more likely due to small sample size, similar to the fact that a small size will not lead to prcise distribution,....

If we picture a word learning trajectory as way one could samples words from the end-state vocabulary, then random ordering corresponds to sparse sampling, 



to the ones obtained with the randomly ranked trajectory. As shown in Figure XX, precision and recall scores in both trajectories are strinkingly similar across development. Thus, despite the fact that languages share some pattern of acquisition above and beyond chance (see Figure, XX), these shared patterns do not influence the higher level organizaton of these words. For example, 





Investigation of the random trajectory allows us to understand whether the developmental patterns are specific to







For real and random: results show that both precision and recall increase across development, suggesting that the conceptual organization undergoes both differentiation for some word pairs (example), and coelescecne for other pairs (examples). 

For within-cluster: precision is high, but recall is low: explain why 

How to explain this change? 

>Assuming a random/real ranking of words:
-Some words may be lamped together initially (even if we force a relatively high number of clusters) because of the smaller vocabulary size? Can we have an example of this? Are the other clsuters empty?
-Some words may be differentiated initially because of the forcing?
-Is there a roleof noise? maybe clustering is just more random for smaller size vocabulary?

>Assuming a with-cluster ranking of words:
-Some words may be lamped together initially (even if we force a relatively high number of clusters): this does not happen, at least while we are still working with the first cluster...that's why precision is generally high
-Some words may be differentiated initially because they 


If precision is lower early on, it means that initial concepts are not differentiated enough, that is, pairs that should belong to different concpets are intially lamped together (false positives), despite the fact that we forced the same number of clsuters in both $\mathcal{C}_t$ and $\mathcal{C}^*$ (Why is this the case? This looks like an artifact of the clustering algorithm with smaller size data? Should discuss this with Isaac. Also, what is an exmaple of a clustering $\mathcal{C}_t$ for a smaller size: are some clusters totally empty?)

-->
