---
title: "Comparing order of acquisition cross-linguistically"
author: "Abdellah Fourtassi"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: default
---
# Preparation

```{r echo=FALSE, message=FALSE, warning=FALSE} 
library(purrr) # fp library
library(readr) # read text data files (csv, etc.)
library(ggplot2) # plotting
library(langcog) # useful things used by Language and Cognition Lab
library(dplyr) # for working with datafram-like objects
library(tidyr) # for data tidying
library(wordbankr) # access to wordbank data
library(stringr) # common string operations
library(broom) # converts R function outputs to tidy data frames
library(igraph) # network analysis and visualization
library(knitr) # general-purpose tool for dynamic report generation
library(kableExtra) # table formatting
library(lemon) # functions for working w/ ggplot2 and knitr extensions
library(tibble) # modern dataframes
library(tidygraph)
library(doParallel)
library(networkD3)
#library(ForceAtlas2)
library(boot)
library(ggthemes)

source(paste(getwd(),"/helpers/all_helper.r",sep = ""), chdir = T)
```

Get cross-linguistic data from Wordbank
```{r}


languages = c("Croatian","Danish","English (American)", "French (Quebecois)", "Italian", "Norwegian", "Russian", "Spanish (Mexican)", "Swedish", "Turkish")


admins <- get_administration_data() %>%
  select(data_id, age, language, form) %>%
  filter(language %in% languages)

items <- get_item_data() %>%
  filter(type == "word", lexical_class == "nouns") %>%
  filter(language %in% languages)

items_by_inst <- split(items, paste(items$language, items$form, sep = "_"))


get_inst_data <- function(inst_items) {
  inst_lang <- unique(inst_items$language)
  inst_form <- unique(inst_items$form)
  inst_admins <- filter(admins, language == inst_lang, form == inst_form)
  get_instrument_data(language = inst_lang ,
                      form = inst_form,
                      administrations = inst_admins,
                      items = inst_items$item_id,
                      iteminfo = inst_items
                      ) %>%
    filter(!is.na(age)) %>%
    mutate(produces = !is.na(value) & value == "produces",
           understands = !is.na(value) & (value == "understands" | value == "produces")) %>%
    select(-value) %>%
    gather(measure, value, produces, understands) %>%
    filter((measure == "understands" & form == "WG") | (measure == "produces" & form == "WS") ) %>%
    mutate(language = inst_lang,
           form = inst_form)
    
}

data_raw <- map(items_by_inst, get_inst_data) 
  
data_all <- bind_rows(data_raw) %>%
  rename(item = num_item_id) %>%
  group_by(language, form, measure,
             lexical_category, lexical_class, uni_lemma,  item, definition, 
             age) %>%
  summarise(num_true = sum(value, na.rm = TRUE),
              num_false = n() - num_true,
              prop = mean(value, na.rm = TRUE)) 

#feather::write_feather(data_all, "saved_data/data_all.feather")

#I am taking out measure to keep only produce
data_all <- feather::read_feather("saved_data/data_all.feather")  

data_all$language <- plyr::mapvalues(data_all$language, 
                                 from = c("Croatian","Danish","English (American)", "French (Quebecois)", "Italian", "Norwegian", "Russian", "Spanish (Mexican)", "Swedish", "Turkish"), 
                                 to = c("Croatian","Danish","English", "French", "Italian", "Norwegian", "Russian", "Spanish", "Swedish", "Turkish"))
  


```

Calculate the age of acquisition
```{r}

fit_inst_measure_uni <- function(inst_measure_uni_data) {
  ages <- min(inst_measure_uni_data$age):max(inst_measure_uni_data$age)
  model <- glm(cbind(num_true, num_false) ~ age, family = "binomial",
               data = inst_measure_uni_data)
  fit <- predict(model, newdata = data.frame(age = ages), se.fit = TRUE)
  aoa <- -model$coefficients[["(Intercept)"]] / model$coefficients[["age"]]
  constants <- inst_measure_uni_data %>%
    ungroup()%>%
    select(language, form, measure, lexical_category, lexical_class, uni_lemma, item, definition) %>%
    distinct()
  
  props <- inst_measure_uni_data %>%
    ungroup() %>%
    select(age, prop)
  
  data.frame(age = ages,
             fit_prop = inv.logit(fit$fit),
             fit_se = fit$se.fit,
             aoa = aoa, language = constants$language,
             form = constants$form,
             measure = constants$measure,
             uni_lemma = constants$uni_lemma,
             item = constants$item,
             definition = constants$definition) %>%
    left_join(props)
}

list_by_item <- data_all %>%
  # make this filtering at the beginning, not here
  split(paste(.$language, .$form, .$measure, .$item))

data_aoa <- map(list_by_item, fit_inst_measure_uni) %>%
  bind_rows()

data_aoa <- data_aoa %>%
  select(language, form, measure, uni_lemma, item, definition, aoa) %>%
  distinct()

#feather::write_feather(data_aoa, "saved_data/data_aoa.feather")

data_aoa <- feather::read_feather("saved_data/data_aoa.feather")

#I am taking out measure to keep only produce
#data_aoa2 <- feather::read_feather("saved_data/aoa_data.feather") %>%
#  rename(definition = defintion) %>%
#  select(language, form, measure, uni_lemma, item, definition, aoa) %>%
#  distinct()

```

Colculate the correlation
```{r}

#Correlation between comprehension and production in each language
#For each pairwise comparision, compute correlation using values unilemmas which are avialable in both languages 

#I should have a double loop?
#For each pair of languages, select the unilemmas and their aoa, rank the unilemmas based on aoa, and calcualte the correlation

# rank unilemma asa function of AoA

data_prod <- data_aoa %>%
  filter(measure =='produces')
  



data_by_lang <- split(data_prod, data_prod$language)


pair_data <- function(lang1){
  
  pair_cor_data <- function (lang2){
  
  lang1_unilemmas <- unique(lang1$uni_lemma)
  lang2_unilemmas <- unique(lang2$uni_lemma)
  uni_intersect <-  intersect(lang1_unilemmas, lang2_unilemmas)
  
  data_lang1 <- lang1 %>%
    filter(!is.na(uni_lemma),
           uni_lemma %in% uni_intersect) %>%
    select(uni_lemma, aoa) %>%
    group_by(uni_lemma) %>%
    summarise(aoa1=mean(aoa))

  
  data_lang2 <- lang2 %>%
    filter(!is.na(uni_lemma),
           uni_lemma %in% uni_intersect) %>%
    select(uni_lemma, aoa) %>%
    group_by(uni_lemma) %>%
    summarise(aoa2=mean(aoa))
  
  both <- data_lang1 %>%
    left_join(data_lang2)
  
  mycorr <- cor.test(rank(both$aoa1), rank(both$aoa2), method = 'kendall')
  val=mycorr$estimate
  p=mycorr$p.value
  language1 = unique(lang1$language) 
  language2 = unique(lang2$language)
  
  data.frame(language1=language1, language2=language2, tau=val, sig = p)
  
}
  map(data_by_lang, pair_cor_data) %>%
    bind_rows()

}


pair_random <- function(lang1){
  
  pair_cor_rand <- function (lang2){
    
  data_lang1 <- lang1 %>%
    filter(!is.na(uni_lemma)) %>%
    select(uni_lemma, aoa) %>%
    group_by(uni_lemma) %>%
    summarise(aoa1=mean(aoa))
  
  order_lang1 <- rank(data_lang1$aoa1)
  order_random <- sample(order_lang1, size = length(order_lang1), replace = FALSE)
  
  mycorr <- cor.test(order_lang1, order_random, method = 'kendall')
  
  val=mycorr$estimate
  p=mycorr$p.value
  language1 = unique(lang1$language) 
  language2 = unique(lang2$language)
  
  data.frame(language1=language1, language2=language2, tau=val, sig = p)
  
}
  map(data_by_lang, pair_cor_rand) %>%
    bind_rows()

}

cor_data <- map(data_by_lang, pair_data) %>%
  bind_rows() %>%
  filter(language1!=language2) %>%
  mutate(data="languages")

cor_random <- map(data_by_lang, pair_random) %>%
  bind_rows() %>%
  mutate(data="random")

cor_all <- cor_data %>%
  bind_rows(cor_random)

```

plots
```{r}

ggplot(cor_all, 
      aes(x = tau, fill=data)) +
  geom_histogram(aes(y = (..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]),
                 binwidth=0.05)+
  #scale_x_log10() +
  theme_few() + 
  theme(aspect.ratio = 0.7, legend.title = element_text(size=8)) +  #facet_grid(Segmentation ~ language)+
  scale_y_continuous(labels = scales::percent)+
  xlab("pairwise correlation") +ylab("Count")



```




## Nouns

We begin with roughly the same data used by Hills. *(TODO fix, get same data)*

```{r warning=FALSE}
# We begin by importing data from wordbank. Age of aquisition (in months, it starts with 16, 17,..) is that at which 50% of children can produce the word. We also do naive triming at this point, this means we are ignoring homophone/polysemy (chiken (food), chiken (animals), etc,.. but we will have to be careful in our choices if we are preparing later for publication.

full_vocab <- make_vocab_dataframe(lang="English (American)",
                                   lang_form = "WS",
                                   lex_class = "nouns") %>%
  trim_all_unilemma() %>%
  trim_all_definition() %>%
  arrange(age) %>%            # keep first age for duplicates
  distinct(uni_lemma, .keep_all = TRUE) %>%
  select(uni_lemma, definition, age, category)
```

The initial vocabulary contains `r nrow(full_vocab)` (compared to Hills' 312).

## Perceptual & Conceptual Features

Features taken from McRae et al. (2005), in which features were collected for 541 nouns from 725 adults, with 30 adults providing  14 features for each noun.

```{r warning=FALSE, message=FALSE}
features <- load_features()
w2v <- load_w2v()

hills_vocab <- full_vocab %>%
  filter(age <= 30) %>%
  filter(uni_lemma %in% features$Concept) # filter to words with features

vocab <- hills_vocab %>%
  filter(uni_lemma %in% w2v$from | uni_lemma %in% w2v$to)
```

To begin our analysis, we only consider the words for which feature data are available, an overlap of `r nrow(hills_vocab)` in our data compared to Hills' 130. Then we filter the `r nrow(hills_vocab) - nrow(vocab)` words for which word2vec data is not available, for a final vocabulary of `r nrow(vocab)`.

## Build Network

Using vocabulary obtained by 30 months and the same McRae features as Hills, we construct a network with edge attributes for perceptual and conceptual features, and for word2vec similarity.

```{r message=FALSE}
# perceptual and conceptual McRae feature types
perceptual_features = c("sound", "taste", "smell", "tactile",
                        "visual-form_and_surface", "visual-motion", "visual-color")
conceptual_features = c("function")

# count shared features between words in vocabulary
perceptual_links <- make_feature_links(vocab = vocab,
                                       feature_types = perceptual_features) %>%
  rename(perceptual = shared)

conceptual_links <- make_feature_links(vocab = vocab,
                                       feature_types = conceptual_features) %>%
  rename(conceptual = shared)

# get word2vec distance between words in vocabulary
w2v_links <- make_w2v_links(vocab = vocab) #TODO still problem w/ vocab matching w2v data

links <- perceptual_links %>%
  full_join(conceptual_links, by=c("from", "to")) %>%
  full_join(w2v_links, by=c("from", "to"))

network <- graph_from_data_frame(links, vertices = vocab, directed = FALSE) %>%
  as_tbl_graph()
```

# Results

## The Network

First we generate a display layout for the network, based on the word2vec data.

```{r}
source(paste(getwd(),"/helpers/all_helper.r",sep = ""), chdir = T)

layout <- layout_network(network %E>% mutate(weights = w2v_similarity))

network <- network %N>%
  mutate(x = layout[,1]) %>%
  mutate(y = layout[,2])
```


We can examine the networks generated by filtering edges with a word2vec similarity threshold.

```{r out.width = '100%', dpi=300}
source(paste(getwd(),"/helpers/all_helper.r",sep = ""), chdir = T)

par(mfrow=c(2, 2), mar=c(2,0.4,0.8,0.4)) # row x col, column first

for (s in seq(0.35, 0.5, 0.05)) {
  plot_network(network %E>% filter(w2v_similarity > s),
               layout = layout,
               labels=TRUE,
               frame=FALSE,
               title=str_glue("Threshold = {s}"))
}

```

## Clustering algorithms

```{r out.width='100%', fig.asp=1/2, dpi=300}
# cluster_edge_betweenness - doesn't cluster reasonably, treats weights as distances?
# 
# cluster_infomap - returns one cluster
# 
# cluster_label_prop - returns one cluster
# 
# cluster_optimal - too computationally intensive for laptop, may not be possible (NP)

source(paste(getwd(),"/helpers/all_helper.r",sep = ""), chdir = T)

par(mfrow=c(1, 2), mar=c(2,0.4,0.8,0.4)) # row x col, column first

feature_network <- network %E>%
  mutate_all(funs(replace(., is.na(.), 0))) %>%
  mutate(features = perceptual + conceptual) %>%
  filter(features > 0)

# cluster_fast_greedy
plot_network(network, layout = layout,
             clusters = cluster_fast_greedy(network,
                                            weights = E(network)$w2v_similarity)$membership,
             labels=TRUE, frame=TRUE)

plot_network(feature_network, layout = layout,
             clusters = cluster_fast_greedy(feature_network,
                                            weights = E(feature_network)$features)$membership,
             labels=TRUE, frame=TRUE)

# cluster_leading_eigen
plot_network(network, layout = layout,
             clusters = cluster_leading_eigen(network,
                                              weights = E(network)$w2v_similarity)$membership,
             labels=TRUE, frame=TRUE)

plot_network(feature_network, layout = layout,
             clusters = cluster_leading_eigen(feature_network,
                                              weights = E(feature_network)$features)$membership,
             labels=TRUE, frame=TRUE)

# cluster_louvain
plot_network(network, layout = layout,
             clusters = cluster_louvain(network,
                                        weights = E(network)$w2v_similarity)$membership,
             labels=TRUE, frame=TRUE)

plot_network(feature_network, layout = layout,
             clusters = cluster_louvain(feature_network,
                                        weights = E(feature_network)$features)$membership,
             labels=TRUE, frame=TRUE)

# cluster_spinglass
plot_network(network, layout = layout,
             clusters = cluster_spinglass(network,
                                          weights = E(network)$w2v_similarity)$membership,
             labels=TRUE, frame=TRUE)

plot_network(feature_network, layout = layout,
             clusters = cluster_spinglass(feature_network,
                                          weights = E(feature_network)$features)$membership,
             labels=TRUE, frame=TRUE)

# cluster_walktrap
plot_network(network, layout = layout,
             clusters = cluster_walktrap(network,
                                         weights = E(network)$w2v_similarity)$membership,
             labels=TRUE, frame=TRUE)

plot_network(feature_network, layout = layout,
             clusters = cluster_walktrap(feature_network,
                                         weights = E(feature_network)$features)$membership,
             labels=TRUE, frame=TRUE)
```

```{r out.width='100%', fig.asp=1/2, dpi=300}
# cluster_walktrap
for (age_limit in seq(16, 30, by=2)) {
  net_at_age <- network %N>% filter(age <= age_limit) %E>% mutate(weights = w2v_similarity)
  
  plot_network(net_at_age,
               clusters = cluster_walktrap(net_at_age,
                                           weights = E(net_at_age)$w2v_similarity)$membership,
               labels = TRUE,
               frame = TRUE)
}
```

```{r}
feature_net <- network %E>% mutate(weights = perceptual) %E>% filter(weights > 0) %N>% filter(!node_is_isolated())
feature_layout <- layout_network(feature_net)

feature_net <- feature_net %N>%
  mutate(x = feature_layout[,1]) %>%
  mutate(y = feature_layout[,2])


```

```{r out.width='100%', fig.asp=1/2, dpi=300}
# cluster_walktrap
for (age_limit in seq(16, 30, by=2)) {
  net_at_age <- feature_net %N>% filter(age <= age_limit) %E>% mutate(weights = perceptual)
  
  plot_network(net_at_age,
               clusters = cluster_walktrap(net_at_age,
                                           weights = E(net_at_age)$perceptual)$membership,
               labels = TRUE,
               frame = TRUE)
}
```
